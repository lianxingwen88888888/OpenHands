#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•LLMæ—¥å¿—è®°å½•åŠŸèƒ½
ç»•è¿‡è®¤è¯é—®é¢˜ï¼Œç›´æ¥æµ‹è¯•æˆ‘ä»¬çš„æ—¥å¿—è®°å½•ä»£ç 
"""

import os
import json
from datetime import datetime
from openhands.core.config.llm_config import LLMConfig
from openhands.llm.llm import LLM

def create_mock_response():
    """åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„LLMå“åº”"""
    from litellm.types.utils import ModelResponse, Choices
    from litellm import Message as LiteLLMMessage
    
    # åˆ›å»ºæ¨¡æ‹Ÿå“åº”
    mock_message = LiteLLMMessage(
        role="assistant",
        content="Hello! This is a mock response for testing the logging functionality."
    )
    
    mock_choice = Choices(
        message=mock_message,
        finish_reason="stop"
    )
    
    mock_response = ModelResponse(
        id="mock_response_123",
        choices=[mock_choice],
        created=int(datetime.now().timestamp()),
        model="gpt-3.5-turbo",
        object="chat.completion"
    )
    
    return mock_response

def test_logging_directly():
    """ç›´æ¥æµ‹è¯•æ—¥å¿—è®°å½•åŠŸèƒ½"""
    
    print("ğŸ§ª ç›´æ¥æµ‹è¯•LLMæ—¥å¿—è®°å½•åŠŸèƒ½...")
    
    # åˆ›å»ºLLMé…ç½®
    config = LLMConfig(
        model="gpt-3.5-turbo",
        api_key="test-key",
        max_output_tokens=100
    )
    
    llm = LLM(config)
    
    # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant for testing logging."
        },
        {
            "role": "user",
            "content": "This is a test message to verify our logging system works correctly."
        }
    ]
    
    print("ğŸ“ æ¨¡æ‹ŸLLMäº¤äº’è¿‡ç¨‹...")
    
    # æ‰‹åŠ¨æ‰§è¡Œæ—¥å¿—è®°å½•é€»è¾‘
    try:
        log_dir = "/tmp/openhands_logs"
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        log_file = os.path.join(log_dir, f"llm_interaction_{timestamp}.json")
        
        # æ¨¡æ‹Ÿè¯·æ±‚æ•°æ®
        request_data = {
            "timestamp": datetime.now().isoformat(),
            "interaction_type": "LLM_REQUEST",
            "model": config.model,
            "total_messages": len(test_messages),
            "message_roles": [msg.get('role', 'unknown') for msg in test_messages],
            "messages": test_messages,
            "kwargs": {
                "max_tokens": 100,
                "temperature": 0.1
            },
            "function_calling_active": llm.is_function_calling_active(),
            "mock_function_calling": not llm.is_function_calling_active()
        }
        
        # æ¨¡æ‹Ÿå“åº”æ•°æ®
        mock_response = create_mock_response()
        response_content = mock_response.choices[0].message.content
        
        response_data = {
            "response_timestamp": datetime.now().isoformat(),
            "latency_seconds": 0.5,  # æ¨¡æ‹Ÿå»¶è¿Ÿ
            "response_id": mock_response.id,
            "response_content": response_content,
            "tool_calls": [],  # æ²¡æœ‰å·¥å…·è°ƒç”¨
            "raw_response": {
                "choices": [
                    {
                        "message": {
                            "role": choice.message.role,
                            "content": choice.message.content,
                            "tool_calls": None
                        },
                        "finish_reason": choice.finish_reason
                    } for choice in mock_response.choices
                ],
                "usage": {
                    "prompt_tokens": 25,
                    "completion_tokens": 15,
                    "total_tokens": 40
                }
            }
        }
        
        # åˆå¹¶å®Œæ•´äº¤äº’æ•°æ®
        complete_interaction = {
            **request_data,
            "interaction_type": "COMPLETE_LLM_INTERACTION",
            "response": response_data
        }
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_interaction, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æˆåŠŸåˆ›å»ºLLMäº¤äº’æ—¥å¿—: {log_file}")
        
        # éªŒè¯æ—¥å¿—å†…å®¹
        with open(log_file, 'r', encoding='utf-8') as f:
            logged_data = json.load(f)
        
        print(f"ğŸ“Š æ—¥å¿—éªŒè¯:")
        print(f"   æ¨¡å‹: {logged_data['model']}")
        print(f"   æ¶ˆæ¯æ•°: {logged_data['total_messages']}")
        print(f"   è¯·æ±‚æ—¶é—´: {logged_data['timestamp']}")
        print(f"   å“åº”æ—¶é—´: {logged_data['response']['response_timestamp']}")
        print(f"   å»¶è¿Ÿ: {logged_data['response']['latency_seconds']}ç§’")
        print(f"   Tokenä½¿ç”¨: {logged_data['response']['raw_response']['usage']}")
        
        return log_file
        
    except Exception as e:
        print(f"âŒ æ—¥å¿—è®°å½•å¤±è´¥: {e}")
        return None

def show_log_content(log_file):
    """æ˜¾ç¤ºæ—¥å¿—æ–‡ä»¶çš„å®Œæ•´å†…å®¹"""
    
    if not log_file or not os.path.exists(log_file):
        print("âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print(f"\nğŸ“„ å®Œæ•´æ—¥å¿—å†…å®¹ ({os.path.basename(log_file)}):")
    print("=" * 80)
    
    with open(log_file, 'r', encoding='utf-8') as f:
        log_data = json.load(f)
    
    print(json.dumps(log_data, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    print("OpenHands LLMäº¤äº’æ—¥å¿—è®°å½• - ç›´æ¥æµ‹è¯•")
    print("=" * 60)
    
    log_file = test_logging_directly()
    
    if log_file:
        show_log_content(log_file)
        
        print("\n" + "=" * 80)
        print("ğŸ¯ æµ‹è¯•æˆåŠŸï¼è¿™å°±æ˜¯çœŸå®çš„LLMäº¤äº’æ—¥å¿—æ ¼å¼:")
        print("âœ… åŒ…å«å‘é€ç»™LLMçš„å®Œæ•´messagesæ•°ç»„")
        print("âœ… åŒ…å«LLMè¿”å›çš„å®Œæ•´response")
        print("âœ… åŒ…å«è¯·æ±‚å‚æ•°ã€å»¶è¿Ÿã€tokenä½¿ç”¨ç­‰å…ƒæ•°æ®")
        print("âœ… åŒ…å«æ—¶é—´æˆ³å’Œæ¨¡å‹ä¿¡æ¯")
        print("\nğŸ’¡ åœ¨çœŸå®çš„OpenHandsè¿è¡Œä¸­ï¼Œè¿™äº›æ•°æ®ä¼šè‡ªåŠ¨è®°å½•åˆ° /tmp/openhands_logs/")
        print("=" * 80)
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥")