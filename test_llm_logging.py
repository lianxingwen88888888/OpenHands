#!/usr/bin/env python3
"""
æµ‹è¯•LLMäº¤äº’è®°å½•åŠŸèƒ½
è¿™ä¸ªè„šæœ¬ä¼šè§¦å‘çœŸå®çš„LLMè°ƒç”¨å¹¶è®°å½•å®Œæ•´çš„è¯·æ±‚-å“åº”
"""

import os
from openhands.core.config.llm_config import LLMConfig
from openhands.llm.llm import LLM

def test_llm_interaction_logging():
    """æµ‹è¯•LLMäº¤äº’è®°å½•"""
    
    print("ğŸš€ æµ‹è¯•LLMäº¤äº’è®°å½•...")
    
    # åˆ›å»ºLLMé…ç½® - ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹
    config = LLMConfig(
        model="gpt-3.5-turbo",  # ä½ å¯ä»¥æ”¹æˆå…¶ä»–æ¨¡å‹
        api_key=os.getenv("OPENAI_API_KEY", "dummy-key"),  # éœ€è¦çœŸå®çš„API key
        max_output_tokens=100,
        temperature=0.1
    )
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = LLM(config)
    
    # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant. Keep responses brief."
        },
        {
            "role": "user", 
            "content": "Hello! This is a test message to trigger LLM logging. Please respond briefly."
        }
    ]
    
    print(f"ğŸ“¤ å‘é€æ¶ˆæ¯åˆ°LLM ({config.model})...")
    print(f"   æ¶ˆæ¯æ•°é‡: {len(test_messages)}")
    print(f"   è§’è‰²: {[msg['role'] for msg in test_messages]}")
    
    try:
        # è°ƒç”¨LLM - è¿™ä¼šè§¦å‘æˆ‘ä»¬çš„æ—¥å¿—è®°å½•
        response = llm.completion(
            messages=test_messages,
            max_tokens=100,
            temperature=0.1
        )
        
        print("âœ… LLMè°ƒç”¨æˆåŠŸ!")
        print(f"ğŸ“¥ å“åº”å†…å®¹: {response.choices[0].message.content}")
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
        log_dir = "/tmp/openhands_logs"
        if os.path.exists(log_dir):
            log_files = [f for f in os.listdir(log_dir) if f.startswith('llm_interaction_')]
            if log_files:
                latest_log = sorted(log_files)[-1]
                log_path = os.path.join(log_dir, latest_log)
                print(f"ğŸ“„ LLMäº¤äº’æ—¥å¿—: {log_path}")
                return log_path
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°LLMäº¤äº’æ—¥å¿—æ–‡ä»¶")
        else:
            print("âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
            
    except Exception as e:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„APIå¯†é’¥")
        
        # å³ä½¿å¤±è´¥ä¹Ÿæ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†æ—¥å¿—
        log_dir = "/tmp/openhands_logs"
        if os.path.exists(log_dir):
            log_files = [f for f in os.listdir(log_dir) if f.startswith('llm_interaction_')]
            if log_files:
                latest_log = sorted(log_files)[-1]
                log_path = os.path.join(log_dir, latest_log)
                print(f"ğŸ“„ éƒ¨åˆ†æ—¥å¿—å¯èƒ½å­˜åœ¨: {log_path}")
                return log_path
    
    return None

def test_with_mock_llm():
    """ä½¿ç”¨æ¨¡æ‹ŸLLMè¿›è¡Œæµ‹è¯•ï¼ˆä¸éœ€è¦çœŸå®API keyï¼‰"""
    
    print("\nğŸ­ ä½¿ç”¨æ¨¡æ‹ŸLLMè¿›è¡Œæµ‹è¯•...")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå“åº”
    from litellm.types.utils import ModelResponse, Choices, Message as LiteLLMMessage
    from litellm import ChatCompletionMessageToolCall
    
    # åˆ›å»ºLLMé…ç½®
    config = LLMConfig(
        model="gpt-3.5-turbo",
        api_key="mock-key",
        max_output_tokens=100
    )
    
    llm = LLM(config)
    
    # æ¨¡æ‹Ÿæ¶ˆæ¯
    test_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello! This is a mock test."}
    ]
    
    print("ğŸ“ å‡†å¤‡æ¨¡æ‹ŸLLMè°ƒç”¨...")
    print("ğŸ’¡ è¿™ä¼šè®°å½•è¯·æ±‚éƒ¨åˆ†ï¼Œä½†å¯èƒ½åœ¨å“åº”æ—¶å¤±è´¥ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")
    
    try:
        response = llm.completion(messages=test_messages)
        print("âœ… æ¨¡æ‹Ÿè°ƒç”¨æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  æ¨¡æ‹Ÿè°ƒç”¨å¤±è´¥ï¼ˆé¢„æœŸçš„ï¼‰: {e}")
        print("ğŸ“ ä½†è¯·æ±‚éƒ¨åˆ†åº”è¯¥å·²ç»è¢«è®°å½•")
    
    # æ£€æŸ¥æ—¥å¿—
    log_dir = "/tmp/openhands_logs"
    if os.path.exists(log_dir):
        log_files = [f for f in os.listdir(log_dir) if f.startswith('llm_interaction_')]
        if log_files:
            latest_log = sorted(log_files)[-1]
            log_path = os.path.join(log_dir, latest_log)
            print(f"ğŸ“„ æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶: {log_path}")
            return log_path
    
    return None

if __name__ == "__main__":
    print("OpenHands LLMäº¤äº’è®°å½•æµ‹è¯•")
    print("=" * 60)
    
    # é¦–å…ˆå°è¯•çœŸå®çš„LLMè°ƒç”¨
    log_file = test_llm_interaction_logging()
    
    # å¦‚æœçœŸå®è°ƒç”¨å¤±è´¥ï¼Œå°è¯•æ¨¡æ‹Ÿè°ƒç”¨
    if not log_file:
        log_file = test_with_mock_llm()
    
    if log_file:
        print(f"\nğŸ¯ æˆåŠŸï¼LLMäº¤äº’å·²è®°å½•åˆ°:")
        print(f"   {log_file}")
        print("\nä½ å¯ä»¥æŸ¥çœ‹è¿™ä¸ªæ–‡ä»¶æ¥çœ‹åˆ°:")
        print("   - å‘é€ç»™LLMçš„å®Œæ•´messagesæ•°ç»„")
        print("   - LLMè¿”å›çš„å®Œæ•´response")
        print("   - è¯·æ±‚å‚æ•°ã€å»¶è¿Ÿã€tokenä½¿ç”¨ç­‰ä¿¡æ¯")
    else:
        print("\nâŒ æ²¡æœ‰ç”Ÿæˆæ—¥å¿—æ–‡ä»¶")
        print("ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥è®¾ç½®æˆ–ç½‘ç»œè¿æ¥")